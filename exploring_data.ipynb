{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import wandb\n",
    "import os\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import GLC23_Dataset_code\n",
    "from GLC23_Dataset_code import GLC23Datasets , GLC23PatchesProviders , GLC23TimeSeriesProviders\n",
    "from GLC23_Dataset_code.GLC23Datasets import RGBNIR_env_Dataset\n",
    "## from models import MLP\n",
    "from util import seed_everything\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glcID</th>\n",
       "      <th>gbifID</th>\n",
       "      <th>observer</th>\n",
       "      <th>datasetName</th>\n",
       "      <th>date</th>\n",
       "      <th>dayOfYear</th>\n",
       "      <th>year</th>\n",
       "      <th>year_ecodatacube_quarter</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>x_EPSG3035</th>\n",
       "      <th>y_EPSG3035</th>\n",
       "      <th>geoUncertaintyInM</th>\n",
       "      <th>speciesId</th>\n",
       "      <th>patchID</th>\n",
       "      <th>timeSerieID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2522017</td>\n",
       "      <td>1.882957e+09</td>\n",
       "      <td>Bjørn Petter Løfall</td>\n",
       "      <td>NOR Species Observation</td>\n",
       "      <td>2018-06-17</td>\n",
       "      <td>168</td>\n",
       "      <td>2018</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.951540</td>\n",
       "      <td>59.25110</td>\n",
       "      <td>4375406.0</td>\n",
       "      <td>4017067.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9343</td>\n",
       "      <td>5982155</td>\n",
       "      <td>2959445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>915820</td>\n",
       "      <td>3.704105e+09</td>\n",
       "      <td>Szokala,Daniel</td>\n",
       "      <td>Masaryk Univ Herbarium</td>\n",
       "      <td>2020-08-23</td>\n",
       "      <td>236</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.678360</td>\n",
       "      <td>43.39697</td>\n",
       "      <td>5345300.0</td>\n",
       "      <td>2341095.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2165</td>\n",
       "      <td>6920543</td>\n",
       "      <td>3900275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2204483</td>\n",
       "      <td>2.634620e+09</td>\n",
       "      <td>Mykyta Peregrym</td>\n",
       "      <td>iNaturalist RG</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>153</td>\n",
       "      <td>2020</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.057780</td>\n",
       "      <td>47.91972</td>\n",
       "      <td>5070487.0</td>\n",
       "      <td>2807349.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7722</td>\n",
       "      <td>6876987</td>\n",
       "      <td>3856654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1892353</td>\n",
       "      <td>2.894783e+09</td>\n",
       "      <td>Vejle Kommune</td>\n",
       "      <td>DEN Environmental Portal</td>\n",
       "      <td>2018-05-16</td>\n",
       "      <td>136</td>\n",
       "      <td>2018</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.337563</td>\n",
       "      <td>55.75870</td>\n",
       "      <td>4279387.0</td>\n",
       "      <td>3628455.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6461</td>\n",
       "      <td>5547194</td>\n",
       "      <td>2523020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3377718</td>\n",
       "      <td>1.884870e+09</td>\n",
       "      <td>Bernt-Gunnar Østerkløft</td>\n",
       "      <td>NOR Species Observation</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>199</td>\n",
       "      <td>2018</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.614180</td>\n",
       "      <td>67.38264</td>\n",
       "      <td>4520783.0</td>\n",
       "      <td>4924950.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>7287</td>\n",
       "      <td>6592021</td>\n",
       "      <td>3570234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4510301</td>\n",
       "      <td>2.823071e+09</td>\n",
       "      <td>Kamil Konowalik</td>\n",
       "      <td>iNaturalist RG</td>\n",
       "      <td>2020-07-18</td>\n",
       "      <td>200</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.369410</td>\n",
       "      <td>51.13202</td>\n",
       "      <td>4835844.0</td>\n",
       "      <td>3139412.0</td>\n",
       "      <td>48.00</td>\n",
       "      <td>8504</td>\n",
       "      <td>6808612</td>\n",
       "      <td>3788047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2873774</td>\n",
       "      <td>2.985083e+09</td>\n",
       "      <td>Kamilla Svingen</td>\n",
       "      <td>NOR Species Observation</td>\n",
       "      <td>2020-11-22</td>\n",
       "      <td>327</td>\n",
       "      <td>2020</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.546747</td>\n",
       "      <td>59.13283</td>\n",
       "      <td>4294996.0</td>\n",
       "      <td>4003638.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4318</td>\n",
       "      <td>5598953</td>\n",
       "      <td>2574870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3019471</td>\n",
       "      <td>2.847813e+09</td>\n",
       "      <td>Jukka Väyrynen</td>\n",
       "      <td>ArtPortalen</td>\n",
       "      <td>2020-08-01</td>\n",
       "      <td>214</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.439410</td>\n",
       "      <td>56.43098</td>\n",
       "      <td>4717999.0</td>\n",
       "      <td>3720884.0</td>\n",
       "      <td>25.00</td>\n",
       "      <td>3911</td>\n",
       "      <td>6749721</td>\n",
       "      <td>3729009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>3849105</td>\n",
       "      <td>2.975628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pl@ntNet automatic</td>\n",
       "      <td>2020-07-14</td>\n",
       "      <td>196</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.702714</td>\n",
       "      <td>50.86739</td>\n",
       "      <td>4300071.0</td>\n",
       "      <td>3084033.0</td>\n",
       "      <td>12.00</td>\n",
       "      <td>9669</td>\n",
       "      <td>5641177</td>\n",
       "      <td>2617157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1246090</td>\n",
       "      <td>2.647760e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pl@ntNet automatic</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>90</td>\n",
       "      <td>2019</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.227480</td>\n",
       "      <td>43.92039</td>\n",
       "      <td>4339312.0</td>\n",
       "      <td>2312336.0</td>\n",
       "      <td>4.29</td>\n",
       "      <td>5776</td>\n",
       "      <td>5784968</td>\n",
       "      <td>2761257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      glcID        gbifID                 observer               datasetName  \\\n",
       "0   2522017  1.882957e+09      Bjørn Petter Løfall   NOR Species Observation   \n",
       "1    915820  3.704105e+09           Szokala,Daniel    Masaryk Univ Herbarium   \n",
       "2   2204483  2.634620e+09          Mykyta Peregrym            iNaturalist RG   \n",
       "3   1892353  2.894783e+09            Vejle Kommune  DEN Environmental Portal   \n",
       "4   3377718  1.884870e+09  Bernt-Gunnar Østerkløft   NOR Species Observation   \n",
       "..      ...           ...                      ...                       ...   \n",
       "95  4510301  2.823071e+09          Kamil Konowalik            iNaturalist RG   \n",
       "96  2873774  2.985083e+09          Kamilla Svingen   NOR Species Observation   \n",
       "97  3019471  2.847813e+09           Jukka Väyrynen               ArtPortalen   \n",
       "98  3849105  2.975628e+09                      NaN        Pl@ntNet automatic   \n",
       "99  1246090  2.647760e+09                      NaN        Pl@ntNet automatic   \n",
       "\n",
       "          date  dayOfYear  year  year_ecodatacube_quarter        lon  \\\n",
       "0   2018-06-17        168  2018                       2.0  10.951540   \n",
       "1   2020-08-23        236  2020                       3.0  22.678360   \n",
       "2   2020-06-01        153  2020                       2.0  20.057780   \n",
       "3   2018-05-16        136  2018                       2.0   9.337563   \n",
       "4   2018-07-18        199  2018                       3.0  14.614180   \n",
       "..         ...        ...   ...                       ...        ...   \n",
       "95  2020-07-18        200  2020                       3.0  17.369410   \n",
       "96  2020-11-22        327  2020                       4.0   9.546747   \n",
       "97  2020-08-01        214  2020                       3.0  16.439410   \n",
       "98  2020-07-14        196  2020                       3.0   9.702714   \n",
       "99  2019-03-31         90  2019                       2.0  10.227480   \n",
       "\n",
       "         lat  x_EPSG3035  y_EPSG3035  geoUncertaintyInM  speciesId  patchID  \\\n",
       "0   59.25110   4375406.0   4017067.0              10.00       9343  5982155   \n",
       "1   43.39697   5345300.0   2341095.0              10.00       2165  6920543   \n",
       "2   47.91972   5070487.0   2807349.0               5.00       7722  6876987   \n",
       "3   55.75870   4279387.0   3628455.0               3.00       6461  5547194   \n",
       "4   67.38264   4520783.0   4924950.0              50.00       7287  6592021   \n",
       "..       ...         ...         ...                ...        ...      ...   \n",
       "95  51.13202   4835844.0   3139412.0              48.00       8504  6808612   \n",
       "96  59.13283   4294996.0   4003638.0               5.00       4318  5598953   \n",
       "97  56.43098   4717999.0   3720884.0              25.00       3911  6749721   \n",
       "98  50.86739   4300071.0   3084033.0              12.00       9669  5641177   \n",
       "99  43.92039   4339312.0   2312336.0               4.29       5776  5784968   \n",
       "\n",
       "    timeSerieID  \n",
       "0       2959445  \n",
       "1       3900275  \n",
       "2       3856654  \n",
       "3       2523020  \n",
       "4       3570234  \n",
       "..          ...  \n",
       "95      3788047  \n",
       "96      2574870  \n",
       "97      3729009  \n",
       "98      2617157  \n",
       "99      2761257  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/sample_data/Presence_only_occurrences/Presences_only_train_sample.csv', sep=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET SEEDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import sample of the data\n",
    "\n",
    "    Set Path of data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/sample_data/\"\n",
    "presence_only_path = data_path + \"Presence_only_occurrences/Presences_only_train_sample.csv\"\n",
    "presence_absence_path = data_path + \"Presence_Absences_occurrences/Presences_Absences_train_sample.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE=1e-3\n",
    "N_EPOCHS = 10\n",
    "BIN_TRESH = 0.1\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(NUM_WORKERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set name of the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'First_run_100_samples'\n",
    "if not os.path.exists(f\"models/{run_name}\"): \n",
    "    os.makedirs(f\"models/{run_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 100 sites, 96 sites\n"
     ]
    }
   ],
   "source": [
    "## TRAIN Set\n",
    "\n",
    "\n",
    "presence_only_df = pd.read_csv(presence_only_path, sep=\";\", header='infer', low_memory=False)\n",
    "\n",
    "\n",
    "train_dataset = RGBNIR_env_Dataset(presence_only_df, env_patch_size=10, rgbnir_patch_size=100)\n",
    "n_species = len(train_dataset.species)\n",
    "print(f\"Training set: {len(train_dataset)} sites, {n_species} sites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 first element of the train_dataset : /n       glcID        gbifID                 observer               datasetName  \\\n",
      "0  2522017  1.882957e+09      Bjørn Petter Løfall   NOR Species Observation   \n",
      "1   915820  3.704105e+09           Szokala,Daniel    Masaryk Univ Herbarium   \n",
      "2  2204483  2.634620e+09          Mykyta Peregrym            iNaturalist RG   \n",
      "3  1892353  2.894783e+09            Vejle Kommune  DEN Environmental Portal   \n",
      "4  3377718  1.884870e+09  Bernt-Gunnar Østerkløft   NOR Species Observation   \n",
      "\n",
      "         date  dayOfYear  year  year_ecodatacube_quarter        lon       lat  \\\n",
      "0  2018-06-17        168  2018                       2.0  10.951540  59.25110   \n",
      "1  2020-08-23        236  2020                       3.0  22.678360  43.39697   \n",
      "2  2020-06-01        153  2020                       2.0  20.057780  47.91972   \n",
      "3  2018-05-16        136  2018                       2.0   9.337563  55.75870   \n",
      "4  2018-07-18        199  2018                       3.0  14.614180  67.38264   \n",
      "\n",
      "   x_EPSG3035  y_EPSG3035  geoUncertaintyInM  speciesId  patchID  timeSerieID  \n",
      "0   4375406.0   4017067.0               10.0       9343  5982155      2959445  \n",
      "1   5345300.0   2341095.0               10.0       2165  6920543      3900275  \n",
      "2   5070487.0   2807349.0                5.0       7722  6876987      3856654  \n",
      "3   4279387.0   3628455.0                3.0       6461  5547194      2523020  \n",
      "4   4520783.0   4924950.0               50.0       7287  6592021      3570234  \n"
     ]
    }
   ],
   "source": [
    "print(f\"5 first element of the train_dataset : /n  {train_dataset.occurrences.head(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: 99 sites, 96 sites\n"
     ]
    }
   ],
   "source": [
    "presence_absence_df = pd.read_csv(presence_absence_path, sep=\";\", header='infer', low_memory=False)\n",
    "val_dataset = RGBNIR_env_Dataset(presence_absence_df, species=train_dataset.species, env_patch_size=10, rgbnir_patch_size=100)\n",
    "print(f\"Validation set: {len(val_dataset)} sites, {len(val_dataset.species)} sites\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dropped_NA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dropped_NA\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dropped_NA' is not defined"
     ]
    }
   ],
   "source": [
    "dropped_NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " number of element in the train dataset : 100\n",
      "first element of the train_dataset :\n",
      "       glcID        gbifID             observer              datasetName  \\\n",
      "0  2522017  1.882957e+09  Bjørn Petter Løfall  NOR Species Observation   \n",
      "\n",
      "         date  dayOfYear  year  year_ecodatacube_quarter       lon      lat  \\\n",
      "0  2018-06-17        168  2018                       2.0  10.95154  59.2511   \n",
      "\n",
      "   x_EPSG3035  y_EPSG3035  geoUncertaintyInM  speciesId  patchID  timeSerieID  \n",
      "0   4375406.0   4017067.0               10.0       9343  5982155      2959445  \n",
      " number of element in the train dataset : 100\n",
      " first element of the val_dataset : \n",
      "     Unnamed: 0    glcID  gbifID observer datasetName        date  dayOfYear  \\\n",
      "0       74594  5688737     NaN      NaN      CBNMed  1587938400        118   \n",
      "\n",
      "   year      lon      lat  x_EPSG3035  y_EPSG3035  geoUncertaintyInM  \\\n",
      "0  2020  6.48976  43.1676   4034867.0   2235568.0               10.0   \n",
      "\n",
      "   speciesId  patchID  timeSerieID  \n",
      "0       1275   116777      3989658  \n"
     ]
    }
   ],
   "source": [
    "print(f\" number of element in the train dataset : {len(train_dataset.occurrences)}\")\n",
    "print(f\"first element of the train_dataset :\\n  {train_dataset.occurrences.head(1)}\")\n",
    "print(f\" number of element in the train dataset : {len(val_dataset.occurrences)}\")\n",
    "print(f\" first element of the val_dataset : \\n  {val_dataset.occurrences.head(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.2745098 , 0.24705882, 0.25098039, ..., 0.28235294,\n",
       "          0.2745098 , 0.28627451],\n",
       "         [0.27058824, 0.25882353, 0.2745098 , ..., 0.28235294,\n",
       "          0.30196078, 0.30196078],\n",
       "         [0.26666667, 0.26666667, 0.28627451, ..., 0.29019608,\n",
       "          0.30196078, 0.30196078],\n",
       "         ...,\n",
       "         [0.29803922, 0.27843137, 0.2745098 , ..., 0.2745098 ,\n",
       "          0.3254902 , 0.35294118],\n",
       "         [0.29019608, 0.29411765, 0.27843137, ..., 0.24313725,\n",
       "          0.2745098 , 0.3254902 ],\n",
       "         [0.28235294, 0.28627451, 0.28627451, ..., 0.22352941,\n",
       "          0.25490196, 0.30196078]],\n",
       " \n",
       "        [[0.30588235, 0.27058824, 0.25882353, ..., 0.27058824,\n",
       "          0.2745098 , 0.28627451],\n",
       "         [0.30196078, 0.28235294, 0.28235294, ..., 0.27058824,\n",
       "          0.30196078, 0.30196078],\n",
       "         [0.29803922, 0.29019608, 0.29411765, ..., 0.29019608,\n",
       "          0.30196078, 0.30196078],\n",
       "         ...,\n",
       "         [0.30980392, 0.29019608, 0.28627451, ..., 0.28235294,\n",
       "          0.31764706, 0.34509804],\n",
       "         [0.29411765, 0.29803922, 0.28235294, ..., 0.26666667,\n",
       "          0.28235294, 0.3254902 ],\n",
       "         [0.28627451, 0.29019608, 0.29019608, ..., 0.25490196,\n",
       "          0.2627451 , 0.30980392]],\n",
       " \n",
       "        [[0.24705882, 0.21568627, 0.20784314, ..., 0.23529412,\n",
       "          0.24313725, 0.25490196],\n",
       "         [0.24313725, 0.22745098, 0.23137255, ..., 0.23529412,\n",
       "          0.27058824, 0.27058824],\n",
       "         [0.24705882, 0.24313725, 0.24313725, ..., 0.25098039,\n",
       "          0.2627451 , 0.2627451 ],\n",
       "         ...,\n",
       "         [0.26666667, 0.24705882, 0.24313725, ..., 0.23137255,\n",
       "          0.27058824, 0.29803922],\n",
       "         [0.2627451 , 0.26666667, 0.25098039, ..., 0.21176471,\n",
       "          0.23921569, 0.28627451],\n",
       "         [0.25490196, 0.25882353, 0.25882353, ..., 0.19607843,\n",
       "          0.21960784, 0.26666667]],\n",
       " \n",
       "        [[0.58823529, 0.5254902 , 0.49019608, ..., 0.49411765,\n",
       "          0.51764706, 0.50588235],\n",
       "         [0.6       , 0.52941176, 0.50588235, ..., 0.50588235,\n",
       "          0.50980392, 0.49019608],\n",
       "         [0.61568627, 0.56862745, 0.53333333, ..., 0.48235294,\n",
       "          0.47058824, 0.46666667],\n",
       "         ...,\n",
       "         [0.5372549 , 0.51764706, 0.5372549 , ..., 0.5372549 ,\n",
       "          0.56470588, 0.54117647],\n",
       "         [0.51764706, 0.52156863, 0.5254902 , ..., 0.50588235,\n",
       "          0.46666667, 0.49803922],\n",
       "         [0.49411765, 0.50588235, 0.52156863, ..., 0.49803922,\n",
       "          0.47843137, 0.50980392]]]),\n",
       " array([[[ 9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01],\n",
       "         [ 9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01],\n",
       "         [ 9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01],\n",
       "         [ 9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01],\n",
       "         [ 9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01],\n",
       "         [ 9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01],\n",
       "         [ 9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01],\n",
       "         [ 9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01],\n",
       "         [ 9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01],\n",
       "         [ 9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01,  9.47176685e-01,  9.47176685e-01,\n",
       "           9.47176685e-01]],\n",
       " \n",
       "        [[ 9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01],\n",
       "         [ 9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01],\n",
       "         [ 9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01],\n",
       "         [ 9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01],\n",
       "         [ 9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01],\n",
       "         [ 9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01],\n",
       "         [ 9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01],\n",
       "         [ 9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01],\n",
       "         [ 9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01],\n",
       "         [ 9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01,  9.75398936e-01,  9.75398936e-01,\n",
       "           9.75398936e-01]],\n",
       " \n",
       "        [[ 6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02],\n",
       "         [ 6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02],\n",
       "         [ 6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02],\n",
       "         [ 6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02],\n",
       "         [ 6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02],\n",
       "         [ 6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02],\n",
       "         [ 6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02],\n",
       "         [ 6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02],\n",
       "         [ 6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02],\n",
       "         [ 6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02,  6.45161290e-02,  6.45161290e-02,\n",
       "           6.45161290e-02]],\n",
       " \n",
       "        [[-1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02],\n",
       "         [-1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02],\n",
       "         [-1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02],\n",
       "         [-1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02],\n",
       "         [-1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02],\n",
       "         [-1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02],\n",
       "         [-1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02],\n",
       "         [-1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02],\n",
       "         [-1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02],\n",
       "         [-1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02, -1.99980000e+02, -1.99980000e+02,\n",
       "          -1.99980000e+02]]]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=2, num_workers=1)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x29dd6fc50>, <torch.utils.data.dataloader.DataLoader object at 0x29dd76090>)\n",
      "Length of train dataloader: 50 batches of 64\n",
      "Length of test dataloader: 2 batches of 64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataloaders: {train_loader, val_loader}\") \n",
    "print(f\"Length of train dataloader: {len(train_loader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(val_loader)} batches of {BATCH_SIZE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 100, 100]) torch.Size([2, 4, 10, 10]) torch.Size([2, 96])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for rgb, env, train_labels_batch in train_loader:\n",
    "    print(rgb.shape, env.shape , train_labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class twoBranchCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_species):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 8, kernel_size=5)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 8, kernel_size=5)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=4)\n",
    "        self.flat2 = nn.Flatten()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(21, 16, kernel_size=3)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(16)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(16, 16, kernel_size=3)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(16)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.flat4 = nn.Flatten()\n",
    "\n",
    "        self.fc5 = nn.Linear(1544, 1024)\n",
    "        self.act5 = nn.ReLU()\n",
    "\n",
    "        self.fc6 = nn.Linear(1024, n_species)\n",
    "\n",
    "    def forward(self, rgb_x, env_x, val=False):\n",
    "        if val: print(rgb_x.shape)\n",
    "        # input 4x100x100 -> output 4x96x96 (k=5)\n",
    "        rgb_x = self.act1(self.batchnorm1(self.conv1(rgb_x)))\n",
    "        # input 8x96x96 -> output 8x48x48 (k=2)\n",
    "        rgb_x = self.pool1(rgb_x)\n",
    "        # input 8x48x48 -> output 8x44x44 (k=5)\n",
    "        rgb_x = self.act2(self.batchnorm2(self.conv2(rgb_x)))\n",
    "        # input 8x44x44 -> output 8x11x11 (k=4)\n",
    "        rgb_x = self.pool2(rgb_x)\n",
    "        # input 8x11x11 -> output 968\n",
    "        rgb_x = self.flat2(rgb_x)\n",
    "        if val: print(rgb_x.shape)\n",
    "\n",
    "        if val: print(env_x.shape)\n",
    "        # input 21x10x10 -> output 16x8x8 (k=3)\n",
    "        env_x = self.act3(self.batchnorm3(self.conv3(env_x)))\n",
    "        # input 16x8x8 -> output 16x6x6 (k=3)\n",
    "        env_x = self.act4(self.batchnorm4(self.conv4(env_x)))\n",
    "        # inpput 16x6x6 -> output 576 (k=2)\n",
    "        env_x = self.flat4(env_x)\n",
    "        if val: print(env_x.shape)\n",
    "\n",
    "        if val: print(x.shape)\n",
    "        # 968 + 576 = 1544\n",
    "        x = torch.cat((rgb_x, env_x), dim=1)\n",
    "        if val: print(x.shape)\n",
    "        # input 1544 -> output 1024\n",
    "        x = self.act5(self.fc5(x))\n",
    "        # input 1024 -> output n_species\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = twoBranchCNN(n_species).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)#, momentum=0.9)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-Up wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanguy-cedoz\u001b[0m (\u001b[33msemester_project_sdm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/wandb/run-20231002_171725-yu3e0h4c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/semester_project_sdm/SDM_project/runs/yu3e0h4c' target=\"_blank\">First_run_100_samples</a></strong> to <a href='https://wandb.ai/semester_project_sdm/SDM_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/semester_project_sdm/SDM_project' target=\"_blank\">https://wandb.ai/semester_project_sdm/SDM_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/semester_project_sdm/SDM_project/runs/yu3e0h4c' target=\"_blank\">https://wandb.ai/semester_project_sdm/SDM_project/runs/yu3e0h4c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='SDM_project', name=run_name, resume='allow', config={\n",
    "        'epochs': N_EPOCHS, 'batch_size': BATCH_SIZE, 'lr': LEARNING_RATE, 'n_species': n_species, \n",
    "        'optimizer':'SGD', 'model': 'cnn_batchnorm_patchsize_20', 'loss': 'BCEWithLogitsLoss', \n",
    "        'env_patch_size': 10, 'rgb_patch_size':100, 'train_data': 'PA'\n",
    "    }) #resume='never',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"models/{run_name}/last.pth\"): \n",
    "        print(f\"Loading model from checkpoint...\")\n",
    "        chekpoint = torch.load(f\"models/{run_name}/last.pth\")\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        min_train_loss = torch.load(f\"models/{run_name}/best_train_loss.pth\")['train_loss']\n",
    "else:\n",
    "    start_epoch = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model and evaluating it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:07<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Exception ignored in: Traceback (most recent call last):\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "<function _ConnectionBase.__del__ at 0x166ebfd80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 21, 3, 3], expected input[2, 4, 10, 10] to have 21 channels, but got 4 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb Cell 32\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_loss_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m rgb, env, labels \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m model(rgb\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(device), env\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X44sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(y_pred, labels\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X44sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# backward pass and weight update\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb Cell 32\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X44sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mif\u001b[39;00m val: \u001b[39mprint\u001b[39m(env_x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X44sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# input 21x10x10 -> output 16x8x8 (k=3)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X44sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m env_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchnorm3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(env_x)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X44sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# input 16x8x8 -> output 16x6x6 (k=3)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X44sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m env_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact4(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchnorm4(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv4(env_x)))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 21, 3, 3], expected input[2, 4, 10, 10] to have 21 channels, but got 4 channels instead"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, N_EPOCHS):\n",
    "        print(f\"EPOCH {epoch}\")\n",
    "\n",
    "        model.train()\n",
    "        train_loss_list = []\n",
    "        for rgb, env, labels in tqdm(train_loader):\n",
    "            y_pred = model(rgb.to(torch.float32).to(device), env.to(torch.float32).to(device))\n",
    "            loss = loss_fn(y_pred, labels.to(torch.float32).to(device))\n",
    "            # backward pass and weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_list.append(loss.cpu().detach())\n",
    "        avg_train_loss = np.array(train_loss_list).mean()\n",
    "        print(f\"\\tTRAIN LOSS={avg_train_loss}\")\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss_list, val_precision_list, val_recall_list, val_f1_list = [], [], [], []\n",
    "        for rgb, env, labels in tqdm(val_loader):\n",
    "            y_pred = model(rgb.to(torch.float32).to(device), env.to(torch.float32).to(dev))#, val=True)\n",
    "            val_loss = loss_fn(y_pred, labels.to(torch.float32).to(device)).cpu().detach()\n",
    "            val_loss_list.append(val_loss)\n",
    "\n",
    "            y_pred = torch.sigmoid(y_pred).cpu().detach().numpy()\n",
    "            y_bin = np.where(y_pred > bin_thresh, 1, 0)\n",
    "            val_precision_list.append(precision_score(labels.T, y_bin.T, average='macro', zero_division=0))\n",
    "            val_recall_list.append(recall_score(labels.T, y_bin.T, average='macro', zero_division=0))\n",
    "            val_f1_list.append(f1_score(labels.T, y_bin.T, average='macro', zero_division=0)) \n",
    "\n",
    "        avg_val_loss = np.array(val_loss_list).mean()\n",
    "        avg_val_precision = np.array(val_precision_list).mean()\n",
    "        avg_val_recall = np.array(val_recall_list).mean()\n",
    "        avg_val_f1 = np.array(val_f1_list).mean()\n",
    "        print(f\"\\tVALIDATION LOSS={avg_val_loss}\\tPRECISION={avg_val_precision}, RECALL={avg_val_recall}, F1-SCORE={avg_val_f1} (threshold={bin_thresh})\")\n",
    "        wandb.log({\n",
    "            \"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss, \n",
    "            \"val_prec\": avg_val_precision, \"val_recall\": avg_val_recall, \"val_f1\": avg_val_f1\n",
    "        })\n",
    "\n",
    "        # model checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }, f\"models/{run_name}/last.pth\") \n",
    "\n",
    "        # save best models\n",
    "        if epoch == 0: \n",
    "            min_train_loss = avg_val_loss\n",
    "            \n",
    "        if avg_train_loss <= min_train_loss:\n",
    "            min_train_loss = avg_train_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_f1': avg_val_f1\n",
    "            }, f\"models/{run_name}/best_train_loss.pth\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: 20 patches -> 20 observations\n",
      "Training set: 79 patches -> 80 observations\n",
      "Training set: 79 sites, 64 sites\n",
      "Validation set: 20 sites, 64 sites\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4giqd176) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">First_run_100_samples</strong> at: <a href='https://wandb.ai/semester_project_sdm/SDM_project/runs/4giqd176' target=\"_blank\">https://wandb.ai/semester_project_sdm/SDM_project/runs/4giqd176</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231002_150626-4giqd176/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4giqd176). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/wandb/run-20231002_162534-eq1lepw2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/semester_project_sdm/geolifeclef23/runs/eq1lepw2' target=\"_blank\">First_run_100_samples</a></strong> to <a href='https://wandb.ai/semester_project_sdm/geolifeclef23' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/semester_project_sdm/geolifeclef23' target=\"_blank\">https://wandb.ai/semester_project_sdm/geolifeclef23</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/semester_project_sdm/geolifeclef23/runs/eq1lepw2' target=\"_blank\">https://wandb.ai/semester_project_sdm/geolifeclef23/runs/eq1lepw2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/GLC23_Dataset_code/GLC23Datasets.py\", line 283, in __getitem__\n    rgb_img = (np.asarray(Image.open(rgb_path)) / 255.0).transpose((2,0,1))\n                          ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/site-packages/PIL/Image.py\", line 3218, in open\n    fp = builtins.open(filename, \"rb\")\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'data/sample_data/SatelliteImages/rgb/32/50/3015032.jpeg'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb Cell 34\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X55sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X55sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m train_loss_list \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X55sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mfor\u001b[39;00m rgb, env, labels \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X55sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m model(rgb\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(dev), env\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(dev))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/exploring_data.ipynb#X55sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(y_pred, labels\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(dev))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/Users/tanguycedoz/Documents/Master/MA3/Semester_project/SDM_project/GLC23_Dataset_code/GLC23Datasets.py\", line 283, in __getitem__\n    rgb_img = (np.asarray(Image.open(rgb_path)) / 255.0).transpose((2,0,1))\n                          ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tanguycedoz/miniconda3/lib/python3.11/site-packages/PIL/Image.py\", line 3218, in open\n    fp = builtins.open(filename, \"rb\")\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'data/sample_data/SatelliteImages/rgb/32/50/3015032.jpeg'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # split presence absence data into validation and training set\n",
    "    # patches are sorted by lat/lon and then the first n_val are chosen \n",
    "    # --> train and val set are geographically separated\n",
    "    presence_absence_df = pd.read_csv(presence_absence_path, sep=\";\", header='infer', low_memory=False)\n",
    "    sorted_patches = presence_absence_df.drop_duplicates(['patchID','dayOfYear']).sort_values(['lat','lon'])\n",
    "    \n",
    "    n_val = round(sorted_patches.shape[0] * 0.2)\n",
    "    val_patches = sorted_patches.iloc[0:n_val] \n",
    "    val_presence_absence = presence_absence_df[(presence_absence_df['patchID'].isin(val_patches['patchID'])) & \n",
    "                             (presence_absence_df['dayOfYear'].isin(val_patches['dayOfYear']))].reset_index(drop=True)\n",
    "    print(f\"Validation set: {n_val} patches -> {val_presence_absence.shape[0]} observations\")\n",
    "    train_patches = sorted_patches.iloc[n_val:]\n",
    "    train_presence_absence = presence_absence_df[(presence_absence_df['patchID'].isin(train_patches['patchID'])) & \n",
    "                             (presence_absence_df['dayOfYear'].isin(train_patches['dayOfYear']))].reset_index(drop=True)\n",
    "    print(f\"Training set: {train_patches.shape[0]} patches -> {train_presence_absence.shape[0]} observations\")\n",
    "\n",
    "    train_dataset = RGBNIR_env_Dataset(train_presence_absence, env_patch_size=10, rgbnir_patch_size=100)\n",
    "    n_species = len(train_dataset.species)\n",
    "    print(f\"Training set: {len(train_dataset)} sites, {n_species} sites\")\n",
    "    val_dataset = RGBNIR_env_Dataset(val_presence_absence, species=train_dataset.species, env_patch_size=10, rgbnir_patch_size=100)\n",
    "    print(f\"Validation set: {len(val_dataset)} sites, {len(val_dataset.species)} sites\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "    \n",
    "    model = twoBranchCNN(n_species).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)#, momentum=0.9)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss() \n",
    "\n",
    "    run = wandb.init(project='geolifeclef23', name=run_name, resume='allow', config={\n",
    "        'epochs': N_EPOCHS, 'batch_size': BATCH_SIZE, 'lr': LEARNING_RATE, 'n_species': n_species, \n",
    "        'optimizer':'SGD', 'model': 'cnn_batchnorm_patchsize_20', 'loss': 'BCEWithLogitsLoss', \n",
    "        'env_patch_size': 10, 'rgb_patch_size':100, 'train_data': 'PA'\n",
    "    }) #resume='never',\n",
    "\n",
    "    if os.path.exists(f\"models/{run_name}/last.pth\"): \n",
    "        print(f\"Loading model from checkpoint...\")\n",
    "        checkpoint = torch.load(f\"models/{run_name}/last.pth\")\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        min_train_loss = torch.load(f\"models/{run_name}/best_train_loss.pth\")['train_loss']\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, N_EPOCHS):\n",
    "        print(f\"EPOCH {epoch}\")\n",
    "\n",
    "        model.train()\n",
    "        train_loss_list = []\n",
    "        for rgb, env, labels in tqdm(train_loader):\n",
    "            y_pred = model(rgb.to(torch.float32).to(dev), env.to(torch.float32).to(dev))\n",
    "            loss = loss_fn(y_pred, labels.to(torch.float32).to(dev))\n",
    "            # backward pass and weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_list.append(loss.cpu().detach())\n",
    "        avg_train_loss = np.array(train_loss_list).mean()\n",
    "        print(f\"\\tTRAIN LOSS={avg_train_loss}\")\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss_list, val_precision_list, val_recall_list, val_f1_list = [], [], [], []\n",
    "        for rgb, env, labels in tqdm(val_loader):\n",
    "            y_pred = model(rgb.to(torch.float32).to(dev), env.to(torch.float32).to(dev))#, val=True)\n",
    "            val_loss = loss_fn(y_pred, labels.to(torch.float32).to(dev)).cpu().detach()\n",
    "            val_loss_list.append(val_loss)\n",
    "\n",
    "            y_pred = torch.sigmoid(y_pred).cpu().detach().numpy()\n",
    "            y_bin = np.where(y_pred > bin_thresh, 1, 0)\n",
    "            val_precision_list.append(precision_score(labels.T, y_bin.T, average='macro', zero_division=0))\n",
    "            val_recall_list.append(recall_score(labels.T, y_bin.T, average='macro', zero_division=0))\n",
    "            val_f1_list.append(f1_score(labels.T, y_bin.T, average='macro', zero_division=0)) \n",
    "\n",
    "        avg_val_loss = np.array(val_loss_list).mean()\n",
    "        avg_val_precision = np.array(val_precision_list).mean()\n",
    "        avg_val_recall = np.array(val_recall_list).mean()\n",
    "        avg_val_f1 = np.array(val_f1_list).mean()\n",
    "        print(f\"\\tVALIDATION LOSS={avg_val_loss}\\tPRECISION={avg_val_precision}, RECALL={avg_val_recall}, F1-SCORE={avg_val_f1} (threshold={bin_thresh})\")\n",
    "        wandb.log({\n",
    "            \"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss, \n",
    "            \"val_prec\": avg_val_precision, \"val_recall\": avg_val_recall, \"val_f1\": avg_val_f1\n",
    "        })\n",
    "\n",
    "        # model checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }, f\"models/{run_name}/last.pth\") \n",
    "\n",
    "        # save best models\n",
    "        if epoch == 0: \n",
    "            min_train_loss = avg_val_loss\n",
    "            \n",
    "        if avg_train_loss <= min_train_loss:\n",
    "            min_train_loss = avg_train_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_f1': avg_val_f1\n",
    "            }, f\"models/{run_name}/best_train_loss.pth\")  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
